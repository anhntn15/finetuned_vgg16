net: "H:/facefinal/model/VGG_train_val.prototxt"
test_iter: 500
test_interval: 500
base_lr: 0.001		# begin training at a learning rate of 0.01 = 1e-2
lr_policy: "step"	# learning rate policy: drop the learning rate in "steps"
					# by a factor of gamma every stepsize iterations
gamma: 0.1
stepsize: 1000	# drop the learning rate every 100K iterations
display: 10
max_iter: 100000
momentum: 0.9
weight_decay: 0.0005
snapshot: 500
snapshot_prefix: "H:/facefinal/model/finetune_"
solver_mode: CPU
